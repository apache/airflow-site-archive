# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
name: Sync GitHub to S3
on: # yamllint disable-line rule:truthy
  workflow_dispatch:
    inputs:
      destination-location:
        description: "The destination location in S3"
        required: true
        type: choice
        options:
          - s3://live-docs-airflow-apache-org/docs/
          - s3://staging-docs-airflow-apache-org/docs/
        default: "s3://live-docs-airflow-apache-org/docs/"
      document-folders:
        description: "Provide any specific package document folders to sync - space separated"
        required: false
        default: "all"
        type: string
      sync-type:
        description: "Perform a full sync or just sync the last commit"
        required: false
        default: "single_commit"
        type: choice
        options:
          - single_commit
          - full_sync
      commit-sha:
        description: "If specified, commit SHA used for single_commit (default is latest commit)"
        required: false
        default: ""
        type: string
      processes:
        description: "Number of processes to use for syncing"
        required: false
        default: "4"
        type: string
jobs:
  github-to-s3:
    name: GitHub to S3
    runs-on: ubuntu-latest
    steps:
      - name: Summarize parameters
        run: |
          echo "Destination location: ${{ inputs.destination-location }}"
          echo "Document folders: ${{ inputs.document-folders }}"
          echo "Sync type: ${{ inputs.sync-type }}"
          echo "Commit SHA: ${{ inputs.commit-sha }}"
          echo "Processes: ${{ inputs.processes }}"

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - uses: actions/checkout@v4
        # Checkout only workflow and scripts directory to run scripts from
        with:
          sparse-checkout: |
            .github
            scripts

      - name: Install AWS CLI v2
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip
          unzip -q /tmp/awscliv2.zip -d /tmp
          rm /tmp/awscliv2.zip
          sudo /tmp/aws/install --update
          rm -rf /tmp/aws/

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@010d0da01d0b5a38af31e9c3470dbfdabdecca3a  # v4.0.1
        with:
          aws-access-key-id: ${{ secrets.DOCS_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DOCS_AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-2

      - name: Remove some stuff we don't need
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf "/usr/local/share/boost"
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"

      - name: "Checkout ${{ github.ref }} ( ${{ github.sha }} ) for scripts"
        uses: actions/checkout@v4
        # Checkout only workflow and scripts directory to run scripts from
        with:
          sparse-checkout: |
            .github
            scripts

      - name: Create /mnt/cloned-airflow-site-archive directory
        run: |
          sudo mkdir -pv /mnt/cloned-airflow-site-archive
          sudo chown -R "${USER}" /mnt/cloned-airflow-site-archive
          ln -v -s /mnt/cloned-airflow-site-archive ./cloned-airflow-site-archive

      - name: Pre-process docs folders
        env:
          DOCUMENTS_FOLDERS: ${{ inputs.document-folders }}
        id: docs-folders-processed
        run: |
          echo "sparse-checkout<<EOF" >> ${GITHUB_OUTPUT}
          if [[ "${DOCUMENTS_FOLDERS}" != "all" ]]; then
            echo "Preprocessing docs folders: ${DOCUMENTS_FOLDERS}"
            folders=""
            sparse_checkout=""
            separator=""
            for folder in ${DOCUMENTS_FOLDERS}; do
              if [[ "${folder}" != apache-airflow-providers* ]]; then
                folders="${folders}${separator}apache-airflow-providers-${folder/./-}"
                echo "docs-archive/apache-airflow-providers-${folder/./-}" >> ${GITHUB_OUTPUT}
              else
                folders="${folders}${separator}${folder}"
                echo "docs-archive/${folder}" >> ${GITHUB_OUTPUT}
              fi
              separator=" "
            done
          else
            folders="all"
            echo "docs-archive" >> ${GITHUB_OUTPUT}
          fi
          echo "EOF" >> ${GITHUB_OUTPUT}
          echo "docs-folders-processed=${folders}"
          echo "docs-folders-processed=${folders}" >> ${GITHUB_OUTPUT}
      - name: >
          Checkout (${{  inputs.commit-sha || github.sha }}) to /mnt/cloned-airflow-site-archive
          with docs: ${{ steps.docs-folders-processed.outputs.docs-folders-processed }}
        uses: actions/checkout@v4
        with:
          path: ./cloned-airflow-site-archive
          fetch-depth: 2
          sparse-checkout: |
            ${{ steps.docs-folders-processed.outputs.sparse-checkout }}
          ref: ${{ inputs.commit-sha || github.sha }}
        if: steps.docs-folders-processed.outputs.docs-folders-processed != 'all'

      - name: >
          Checkout (${{  inputs.commit-sha || github.sha }}) to /mnt/cloned-airflow-site-archive (whole repo)
        uses: actions/checkout@v4
        with:
          path: ./cloned-airflow-site-archive
          fetch-depth: 2
          ref: ${{ inputs.commit-sha || github.sha }}
        if: steps.docs-folders-processed.outputs.docs-folders-processed == 'all'

      - name: >
          Syncing ${{ inputs.commit-sha || github.sha }}: ${{ inputs.destination-location }}:
          ${{ inputs.sync-type }} ${{ steps.docs-folders-processed.outputs.docs-folders-processed }}
          wih parallel aws cli methods = ${{ inputs.processes }}
        env:
          COMMIT_SHA: ${{ inputs.commit-sha || github.sha }}
          SYNC_TYPE: ${{ inputs.sync-type }}
          PROCESSES: ${{ inputs.processes }}
          DOCUMENTS_FOLDERS: ${{ steps.docs-folders-processed.outputs.docs-folders-processed }}
          DESTINATION_LOCATION: ${{ inputs.destination-location }}
        run: |
          # show what's being run
          set -x
          if [[ "${SYNC_TYPE}" == "single_commit" ]]; then
            echo "Syncing ${COMMIT_SHA}"
          else
            echo "Syncing whole repo"
          fi
          ls -la /mnt/cloned-airflow-site-archive/*
          python3 -m pip install uv
          # we run inputs.processes aws cli commands - each command uploading files in parallel
          # that seems to be the fastest way to upload files to S3
          aws configure set default.s3.max_concurrent_requests 10
          uv run ./scripts/github_to_s3.py \
            --bucket-path ${DESTINATION_LOCATION} \
            --local-path /mnt/cloned-airflow-site-archive/docs-archive \
            --document-folders "${DOCUMENTS_FOLDERS}" \
            --commit-sha ${COMMIT_SHA} --sync-type ${SYNC_TYPE} \
            --processes ${PROCESSES}
          
